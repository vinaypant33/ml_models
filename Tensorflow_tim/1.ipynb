{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67001a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd13e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from 'c:\\\\Users\\\\vinay\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db45678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c539d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has vectors and does the matrix computation :  it does 2 , 3 ,  4 and sometiems 5 dimensions ( 4 dimentions for imgaes )\n",
    "# vectors and Matrices and the tf does computation for the tensors \n",
    "# Each tensor has a data type and a shape  : they sometimes has flot string etc. \n",
    "# shape is the representaiton for the dimension for  the data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288cdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors : \n",
    "string  = tf.Variable(\"this is a string \" , tf.string)\n",
    "number  = tf.Variable(33 , tf.int16)\n",
    "floating  = tf.Variable(3.567 , tf.float16)\n",
    "\n",
    "# rank Degree of tensors:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8874d",
   "metadata": {},
   "source": [
    "Another word for rank is the degree , These terms simple mean the number of dimensions involved in the tensor what we created above is a tensor of rank 0 also knows as scaler. \n",
    "For higher degree tensors we can define them as  : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a71bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1_tensor  = tf.Variable(['Test_1'] , tf.string)\n",
    "rank2_tensor  = tf.Variable([\"Test\" , \"okay\"] , [\"Test\" , \"Yes\"] , tf.string)\n",
    "\n",
    "# now to check the rank of the tensor we can use the code as  : \n",
    "rank2_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724e3ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9eb683",
   "metadata": {},
   "source": [
    "Make sure that there are same amount of tensors in the code while defining the tensors and they should be uniform. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a375de5",
   "metadata": {},
   "source": [
    "# Changing the shape of the tensors : \n",
    "\n",
    "Sometimes we have to change the shape of the tensors so that they match when there are differnet shape and flat it out to make it the same dimension / shape as other tensor or ......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41c861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = tf.ones([1,2,3])\n",
    "\n",
    "tensor_2 = tf.reshape(tensor1 , [2 ,3 ]) # reshape existing data from tensor 1 and then adding 2 and 3 in that one as well \n",
    "\n",
    "# we can use this method to reshape it again and again\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae327cb6",
   "metadata": {},
   "source": [
    "# Types of tensors : \n",
    "1.variables \n",
    "2. Constant\n",
    "3. Placeholder\n",
    "4. Sparse Tensor \n",
    "\n",
    "apart from variable other are immutable  \n",
    "\n",
    "To evaluate a tensor we need to make a session since tensors represent a partially complte computation we will sometimes need to run whats callsed a sssion to evaluate the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c8293a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[0;32m      2\u001b[0m     tensor\u001b[38;5;241m.\u001b[39meval()   \u001b[38;5;66;03m# tensor is the name of the tensro \u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tensor.eval()   # tensor is the name of the tensro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d24ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c782fff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db13a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0.]]]]], shape=(5, 5, 5, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t  = tf.zeros([5,5,5,5,5])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3d27d",
   "metadata": {},
   "source": [
    "## Core Learning Algorithms :  \n",
    "\n",
    "1. Linear Regression\n",
    "2. Classification\n",
    "3. Clustering\n",
    "4. Hidden Markov Models \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd62647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blunt Code for the linear regression using tensorflow :  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "import urllib\n",
    "\n",
    "import tensorflow.compat.v2.feature_column as fc   # used for the tablular data for the feature columns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifically for the tabular data and linear regression  : \n",
    "\n",
    "dftrain  = pd.read_csv('data frame location ') # Training data \n",
    "dfeval  = pd.read_csv('data frame antoher testing dta set ') # Testing data\n",
    "\n",
    "y_train  = dftrain.pop('Survived')\n",
    "y_eval = dfeval.pop(\"Survived\")  # This is the column name in the dummy data \n",
    "\n",
    "CATEGORICAL_COLUMNS  = ['']\n",
    "\n",
    "NUMERICAL_COLUMNS  = ['']\n",
    "\n",
    "feature_columns  =  []\n",
    "\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocabulary = dftrain[feature_name].unique() # Gets the list of all unique values from given feature column\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name , dtype=tf.float32))\n",
    "\n",
    "print(feature_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a238d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dftrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdftrain\u001b[49m[feature_name]\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dftrain' is not defined"
     ]
    }
   ],
   "source": [
    "dftrain[feature_name].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcddf4e",
   "metadata": {},
   "source": [
    "For small datset we can load the dataset in one go but if the dataset is large we need to laod the dataset in batches in the model and to make the batches : \n",
    "\n",
    "for example in this one we maek batches of 32 : \n",
    "epochs :  is the iterations ( how many times the model gonna see the same data  - this makes the model to learn about the datsaet but with each iteration the model sees the data but in other sequence )\n",
    "\n",
    "Input Function :  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027c1710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dftrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ds \u001b[38;5;66;03m# Return a batch of the dataset : \u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_function \u001b[38;5;66;03m# Retur a function object for use : \u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train_input_fn  \u001b[38;5;241m=\u001b[39m make_input_fn(\u001b[43mdftrain\u001b[49m , y_train) \u001b[38;5;66;03m# Here we will call the input function that was returned to us to get a \u001b[39;00m\n\u001b[0;32m     16\u001b[0m eval_input_fn \u001b[38;5;241m=\u001b[39m make_input_fn(dfeval , y_eval , num_epoches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m , shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m linear_est \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mLinearClassifier(feature_columns\u001b[38;5;241m=\u001b[39mfeature_columns) \u001b[38;5;66;03m# Train the model for linear classification  / Regression \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dftrain' is not defined"
     ]
    }
   ],
   "source": [
    "# Input FUnciton : way to define the data how the data is gonna be broken in epochs  : it takes the data and encodes it in tf.data.Dataset object : casue tensorflow requres the input in this form to ake  a model from it \n",
    "\n",
    "\n",
    "def make_input_fn(data_df , label_df , num_epoches  = 10 , shuffle  = True  , batch_size = 32):\n",
    "    def input_function(): # THe innner function : \n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df) , label_df))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000) # Randomize order of data : \n",
    "            ds = ds.batch(batch_size).repeat(num_epoches) # Split dataset into batches of 32 and repeat process for number of epoches \n",
    "            return ds # Return a batch of the dataset : \n",
    "    return input_function # Retur a function object for use : \n",
    "\n",
    "\n",
    "\n",
    "train_input_fn  = make_input_fn(dftrain , y_train) # Here we will call the input function that was returned to us to get a \n",
    "eval_input_fn = make_input_fn(dfeval , y_eval , num_epoches=1 , shuffle=False)\n",
    "\n",
    "\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns) # Train the model for linear classification  / Regression \n",
    " \n",
    "result  = linear_est.evaluate(eval_input_fn) # get model meterics / stats by testing on testing data.\n",
    "\n",
    "Clear_output()  # Clears the console output \n",
    "\n",
    "print(result['accuracy']) # The result variable is simple a dict of stas about our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad167cf7",
   "metadata": {},
   "source": [
    "They are used to make large batches of data and not good to make predictions for only one set of data. \n",
    "Better to use the tensorflow for large dataset rathen than using it for small dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to have an inpput function  : \n",
    "\n",
    "result = linear_est.predict(eval_input_fn) # we need input function to preduict as well just like we need to train the model\n",
    "print(result)\n",
    "\n",
    "# to get the specific result we can take it as  : \n",
    "print(result[0]['probabilites'][1]) \n",
    "\n",
    "print(dfeval.loc[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0939d14",
   "metadata": {},
   "source": [
    "We also can do the classification similar to we have done for the regression : \n",
    "\n",
    "\n",
    "classification   : diferentiating bwtween the classes and defining of they belong to a certain group.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['sepanlenght' , 'sepalwidth' , 'petallength' , 'petalwidth' , 'species']\n",
    "SPECIES  = ['setosa' , 'versicolor' , 'virginia']\n",
    "\n",
    "train_path = tf.keras.utils.get_file ('iris_training.csv' , \"https://Next url\")\n",
    "test_path  = tf.keras.utils.get_file('test file name with the url ') \n",
    "\n",
    "\n",
    "train = pd.read_csv(train_path , names=CSV_COLUMN_NAMES , header=0)\n",
    "test = pd.read_csv(test_path , names = CSV_COLUMN_NAMES , header = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f65d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() # To check the data and get idea of how it works. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f965c0",
   "metadata": {},
   "source": [
    "Rest of the steps would be the same as other models do just we have to focus on the input function  : \n",
    "in here we dont have the epochs and the batch size is different\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(features  , labels   ,training = True , batch_size = 256):\n",
    "    # Converts the inputs to a dataset : \n",
    "    dataset = tf.data.Datset.from_tensor_slices((dict(features) , labels))\n",
    "    if training :\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13337e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are all the numerical feature columns then we dont have to make the multiple for loops  : \n",
    "feature_columns  = []\n",
    "for key in train.keys():\n",
    "    feature_columns.append(tf.feature_column.numeric_column(key = key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2acf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model : for the classification model  :  same as the liner model we used it was pre built   :\n",
    "# for ther are two type DNN classifier ( deep learnign classifier ) , LinearClassifier ( used for the most liner data ) usually deep learning classifier is used most of the time \n",
    "\n",
    "# Build DNN with 2 hidden layers with 20 and 10 hidden nodes each  \n",
    "\n",
    "classifier   = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    # two hidden layers fo 30 and 10 nodes in each node respectively  \n",
    "    hidden_units=[30 , 10],\n",
    "    # The model must choose between 3 classes \n",
    "    n_classes=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(\n",
    "    input_fn  = lambda :  input_fn(train , train_y , training  = True ),\n",
    "    steps  = 5000 # simiilar to epoch but it has steps but the meaning it it goes to a dataset until it finds 5000 values or things like that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31ef15",
   "metadata": {},
   "source": [
    "While running the train it finds the loss and prints it and over the period of time the losses should decrease and which means the model is learning but if the count is still high then we need to make somehting so that the losses are reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(input_fn  = lambda : input_fn(test , test_y , training= False))\n",
    "print(f\"Accurary {**eval_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd3267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ef43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad639e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf5dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb381db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053abb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
